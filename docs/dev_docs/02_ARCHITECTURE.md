# System Architecture

This document provides a detailed overview of the P3_TB system architecture, explaining how components interact and why specific design decisions were made.

## High-Level Architecture

The system follows a client-server architecture with transparent syscall interception:

```
┌───────────────────────────────────────────────────────────────────┐
│                         User Program                               │
│                    (Unmodified Binary)                             │
│                                                                     │
│  int fd = open("/tmp/file.txt", O_RDONLY);  ← Normal syscalls     │
│  read(fd, buffer, 1024);                                           │
│  close(fd);                                                        │
└───────────────────┬───────────────────────────────────────────────┘
                    │ Function calls go to libc
                    ↓
┌───────────────────────────────────────────────────────────────────┐
│              LD_PRELOAD Interception Layer                         │
│                    (intercept.so)                                  │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  Interceptors (src/intercept/*.h)                           │ │
│  │  • open()/open64() → intercept_open.h                       │ │
│  │  • close()         → intercept_close.h                      │ │
│  │  • read()          → intercept_read.h                       │ │
│  │  • write()         → intercept_write.h                      │ │
│  │                                                              │ │
│  │  Each interceptor:                                          │ │
│  │  1. Checks reentry guard                                    │ │
│  │  2. Gets RPC client connection                              │ │
│  │  3. Packages arguments into XDR structure                   │ │
│  │  4. Calls RPC stub (generated by rpcgen)                    │ │
│  │  5. Returns result with proper errno                        │ │
│  └─────────────────────────────────────────────────────────────┘ │
│                              ↓                                     │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  RPC Client Manager (src/rpc_client.c)                      │ │
│  │  • Maintains per-thread CLIENT* handle                      │ │
│  │  • Lazy connection initialization                           │ │
│  │  • Transport selection (UNIX vs TCP)                        │ │
│  │  • Connection lifecycle management                          │ │
│  └─────────────────────────────────────────────────────────────┘ │
│                              ↓                                     │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  Generated RPC Client Stubs (protocol_clnt.c)               │ │
│  │  • syscall_open_1()                                         │ │
│  │  • syscall_close_1()                                        │ │
│  │  • syscall_read_1()                                         │ │
│  │  • syscall_write_1()                                        │ │
│  │                                                              │ │
│  │  Each stub:                                                 │ │
│  │  • Serializes request with XDR                              │ │
│  │  • Sends to server via CLIENT handle                        │ │
│  │  • Receives response                                        │ │
│  │  • Deserializes response with XDR                           │ │
│  └─────────────────────────────────────────────────────────────┘ │
└───────────────────────┬───────────────────────────────────────────┘
                        │ RPC over transport layer
                        ↓
        ┌───────────────────────────────┐
        │     Transport Layer           │
        │  ┌─────────────────────────┐  │
        │  │  UNIX Domain Socket     │  │
        │  │  /tmp/p3_tb             │  │
        │  └─────────────────────────┘  │
        │           OR                  │
        │  ┌─────────────────────────┐  │
        │  │  TCP Socket             │  │
        │  │  localhost:9999         │  │
        │  └─────────────────────────┘  │
        └───────────────┬───────────────┘
                        │ Network packets (XDR-encoded)
                        ↓
┌───────────────────────────────────────────────────────────────────┐
│                       RPC Server                                   │
│                  (syscall_server binary)                           │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  Transport Listener (src/rpc_server.c:main)                 │ │
│  │  • Creates socket based on RPC_TRANSPORT env var            │ │
│  │  • UNIX: Manual socket/bind/listen/accept loop              │ │
│  │  • TCP: svctcp_create() + portmapper registration           │ │
│  │  • Dispatches to generated RPC dispatcher                   │ │
│  └─────────────────────────────────────────────────────────────┘ │
│                              ↓                                     │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  Generated RPC Dispatcher (protocol_svc.c)                  │ │
│  │  • syscall_prog_1() - Main dispatcher                       │ │
│  │  • Receives requests                                        │ │
│  │  • Deserializes with XDR                                    │ │
│  │  • Routes to appropriate *_svc function                     │ │
│  │  • Serializes response                                      │ │
│  │  • Sends back to client                                     │ │
│  └─────────────────────────────────────────────────────────────┘ │
│                              ↓                                     │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  Server Implementation (src/rpc_server.c)                   │ │
│  │  • syscall_open_1_svc()   - Handles open requests           │ │
│  │  • syscall_close_1_svc()  - Handles close requests          │ │
│  │  • syscall_read_1_svc()   - Handles read requests           │ │
│  │  • syscall_write_1_svc()  - Handles write requests          │ │
│  │                                                              │ │
│  │  Each handler:                                              │ │
│  │  1. Receives XDR-deserialized request                       │ │
│  │  2. Maps client_fd → server_fd                              │ │
│  │  3. Executes actual syscall                                 │ │
│  │  4. Captures result and errno                               │ │
│  │  5. For open: creates new FD mapping                        │ │
│  │  6. For close: removes FD mapping                           │ │
│  │  7. Returns response (auto-serialized by dispatcher)        │ │
│  └─────────────────────────────────────────────────────────────┘ │
│                              ↓                                     │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │  FD Mapping Table (src/rpc_server.c)                        │ │
│  │  • fd_mapping[MAX_FDS] array                                │ │
│  │  • client_fd (index) → server_fd (value)                    │ │
│  │  • add_fd_mapping() - Creates new mapping                   │ │
│  │  • remove_fd_mapping() - Removes mapping on close           │ │
│  │  • get_server_fd() - Translates client_fd → server_fd       │ │
│  └─────────────────────────────────────────────────────────────┘ │
└───────────────────────┬───────────────────────────────────────────┘
                        │ Actual syscalls
                        ↓
┌───────────────────────────────────────────────────────────────────┐
│                    Operating System Kernel                         │
│                     Real Filesystem                                │
│  • open() creates real file descriptors                           │
│  • read()/write() perform actual I/O                              │
│  • close() releases resources                                     │
└───────────────────────────────────────────────────────────────────┘
```

## Component Breakdown

### 1. User Program Layer

**Purpose**: The target application we want to intercept

**Characteristics**:
- Unmodified binary (no source code changes)
- Makes normal libc syscall wrappers (open, read, write, close)
- Completely unaware of interception happening

**Example**:
```c
// Any program, e.g., cat, ls, or custom application
int fd = open("/tmp/file.txt", O_RDONLY);
if (fd >= 0) {
    char buf[1024];
    ssize_t n = read(fd, buf, sizeof(buf));
    write(STDOUT_FILENO, buf, n);
    close(fd);
}
```

### 2. Interception Layer (intercept.so)

**Purpose**: Transparent syscall hijacking without modifying target program

**Key Files**:
- `src/intercept/intercept_open.h`
- `src/intercept/intercept_close.h`
- `src/intercept/intercept_read.h`
- `src/intercept/intercept_write.h`
- `src/rpc_client.c` (connection management)

**Mechanism**: LD_PRELOAD technique

When a program is run with:
```bash
LD_PRELOAD=./build/intercept.so ./my_program
```

The dynamic linker loads `intercept.so` *before* libc, so our interceptor functions are called instead of the real libc functions.

**Interception Pattern** (used in all interceptors):
```c
int open(const char *pathname, int flags, ...) {
    // Step 1: Reentry guard - prevent infinite recursion
    if (in_open_intercept || is_rpc_in_progress()) {
        return syscall(SYS_open, pathname, flags, mode);
    }

    in_open_intercept = 1;  // Mark as inside interceptor

    // Step 2: Get RPC client connection (lazy initialized, per-thread)
    CLIENT *client = get_rpc_client();

    int result = -1;
    if (client != NULL) {
        // Step 3: Package arguments into XDR structure
        open_request req = {
            .path = (char*)pathname,
            .flags = flags,
            .mode = mode
        };

        // Step 4: Disable interception during RPC call
        rpc_in_progress = 1;

        // Step 5: Call RPC stub (communicates with server)
        open_response *res = syscall_open_1(&req, client);

        rpc_in_progress = 0;

        // Step 6: Extract result and errno
        if (res != NULL) {
            result = res->result;
            errno = res->err;
        }
    } else {
        // Fallback: call real syscall if RPC unavailable
        result = syscall(SYS_open, pathname, flags, mode);
    }

    in_open_intercept = 0;  // Clear guard
    return result;
}
```

**Thread Safety**:
- Each interceptor uses `__thread` local reentry guard
- RPC client connection is per-thread (`__thread CLIENT *rpc_client`)
- Global `rpc_in_progress` flag prevents recursion during RPC initialization

### 3. RPC Client Layer

**Purpose**: Manage persistent RPC connections and abstract transport details

**Key File**: `src/rpc_client.c`

**Key Function**: `get_rpc_client()`

```c
CLIENT *get_rpc_client(void) {
    // Return existing connection if available
    if (rpc_client != NULL) {
        return rpc_client;
    }

    // Prevent recursion during initialization
    if (in_rpc_init) {
        return NULL;
    }

    in_rpc_init = 1;
    rpc_in_progress = 1;  // CRITICAL: Disable interceptors!

    // Select transport based on environment variable
    transport_type_t transport = get_transport_type();

    if (transport == TRANSPORT_UNIX) {
        // UNIX domain socket
        int sock = socket(AF_UNIX, SOCK_STREAM, 0);
        struct sockaddr_un server_addr = {
            .sun_family = AF_UNIX,
        };
        strncpy(server_addr.sun_path, UNIX_SOCKET_PATH, ...);

        connect(sock, ...);

        // Create RPC client from connected socket
        struct netbuf svcaddr = {
            .len = sizeof(server_addr),
            .maxlen = sizeof(server_addr),
            .buf = (char *)&server_addr
        };

        rpc_client = clnt_vc_create(sock, &svcaddr,
                                     SYSCALL_PROG, SYSCALL_VERS, 0, 0);
    } else {
        // TCP socket (requires rpcbind)
        rpc_client = clnt_create(TCP_HOST, SYSCALL_PROG, SYSCALL_VERS, "tcp");
    }

    rpc_in_progress = 0;
    in_rpc_init = 0;

    return rpc_client;
}
```

**Design Rationale**:
- **Persistent connections**: Avoid overhead of connection setup per syscall
- **Per-thread**: Each thread has independent RPC client (thread-safe)
- **Lazy initialization**: Connection created on first syscall
- **Recursion prevention**: Critical during initialization when RPC library internally calls syscalls

### 4. Generated RPC Stubs (protocol_clnt.c)

**Purpose**: Auto-generated client-side RPC functions

**Generated by**: `rpcgen -C protocol.x`

**Example Stub** (simplified view):
```c
open_response *syscall_open_1(open_request *argp, CLIENT *clnt) {
    static open_response clnt_res;

    memset(&clnt_res, 0, sizeof(clnt_res));

    // RPC call: serialize request, send, receive, deserialize response
    if (clnt_call(clnt, SYSCALL_OPEN,
                  (xdrproc_t) xdr_open_request, (caddr_t) argp,
                  (xdrproc_t) xdr_open_response, (caddr_t) &clnt_res,
                  TIMEOUT) != RPC_SUCCESS) {
        return NULL;
    }

    return &clnt_res;
}
```

**Key Points**:
- Uses static storage (result must persist after return for caller)
- `clnt_call()` handles all network communication
- XDR functions (xdr_open_request, xdr_open_response) handle serialization
- Returns NULL on RPC failure

### 5. Transport Layer

**Purpose**: Abstract communication channel between client and server

**Supported Transports**:

| Transport | Configuration | Use Case | Requires rpcbind? |
|-----------|--------------|----------|-------------------|
| UNIX Domain Socket | `RPC_TRANSPORT=unix` (default) | Same machine, fastest | No |
| TCP Socket | `RPC_TRANSPORT=tcp` | Remote machines, network | Yes |

**Configuration**: `src/transport_config.h`
```c
#define UNIX_SOCKET_PATH "/tmp/p3_tb"
#define TCP_HOST "localhost"
#define TCP_PORT 9999

transport_type_t get_transport_type(void) {
    const char *env = getenv("RPC_TRANSPORT");
    if (env && strcasecmp(env, "tcp") == 0) {
        return TRANSPORT_TCP;
    }
    return TRANSPORT_UNIX;  // Default
}
```

**Wire Format**: XDR binary encoding
- All data in network byte order (big-endian)
- Strings length-prefixed
- Padding to 4-byte boundaries

### 6. RPC Server Layer

**Purpose**: Receive RPC requests and execute syscalls on behalf of clients

**Key File**: `src/rpc_server.c`

**Main Function** (simplified):
```c
int main(int argc, char *argv[]) {
    init_fd_mapping();

    transport_type_t transport = get_transport_type();

    if (transport == TRANSPORT_UNIX) {
        // UNIX socket: Manual accept loop
        int listen_sock = socket(AF_UNIX, SOCK_STREAM, 0);
        struct sockaddr_un addr = { .sun_family = AF_UNIX };
        strncpy(addr.sun_path, UNIX_SOCKET_PATH, ...);

        unlink(UNIX_SOCKET_PATH);  // Remove stale socket
        bind(listen_sock, ...);
        listen(listen_sock, 5);

        printf("[Server] Listening on UNIX socket: %s\n", UNIX_SOCKET_PATH);

        while (1) {
            int client_sock = accept(listen_sock, NULL, NULL);

            // Create RPC service handle for this client
            SVCXPRT *transp = svcfd_create(client_sock, 0, 0);

            // Register dispatcher for SYSCALL_PROG
            svc_register(transp, SYSCALL_PROG, SYSCALL_VERS,
                        syscall_prog_1, 0);

            // Handle requests from this client
            svc_run();  // Blocks until client disconnects

            // Cleanup
            svc_unregister(SYSCALL_PROG, SYSCALL_VERS);
            svc_destroy(transp);
        }
    } else {
        // TCP socket: Use portmapper
        SVCXPRT *transp = svctcp_create(RPC_ANYSOCK, 0, 0);

        svc_register(transp, SYSCALL_PROG, SYSCALL_VERS,
                    syscall_prog_1, IPPROTO_TCP);

        printf("[Server] Listening on TCP port...\n");
        svc_run();  // Never returns
    }
}
```

**Server Implementations**:

Each `*_svc` function follows this pattern:

```c
open_response* syscall_open_1_svc(open_request *req, struct svc_req *rqstp) {
    static open_response res;  // Must be static!

    // Execute the actual syscall
    int server_fd = open(req->path, req->flags, req->mode);

    if (server_fd >= 0) {
        // Success: create FD mapping
        int client_fd = add_fd_mapping(server_fd);
        res.fd = client_fd;
        res.result = client_fd;
        res.err = 0;

        printf("[Server] open('%s') -> server_fd=%d, client_fd=%d\n",
               req->path, server_fd, client_fd);
    } else {
        // Failure: return error
        res.fd = -1;
        res.result = -1;
        res.err = errno;

        printf("[Server] open('%s') failed: %s\n", req->path, strerror(errno));
    }

    return &res;  // Dispatcher automatically serializes and sends
}
```

**Critical Points**:
- Return value must be pointer to static/allocated memory
- errno captured immediately after syscall
- FD mapping happens here (see next section)

### 7. File Descriptor Mapping

**Purpose**: Translate between client-side FDs and server-side FDs

**Why Needed?**

Client and server are separate processes with independent FD tables:

```
Client Process                    Server Process
┌─────────────┐                  ┌─────────────┐
│ FD 0: stdin │                  │ FD 0: stdin │
│ FD 1: stdout│                  │ FD 1: stdout│
│ FD 2: stderr│                  │ FD 2: stderr│
│ FD 3: ???   │ ←───mapping───→  │ FD 5: file  │
│ FD 4: ???   │ ←───mapping───→  │ FD 7: file  │
└─────────────┘                  └─────────────┘
```

Client thinks it has FD 3, but server's actual FD is 5. We need bidirectional mapping.

**Implementation** (src/rpc_server.c):
```c
#define MAX_FDS 1024

// Maps client_fd (index) → server_fd (value)
static int fd_mapping[MAX_FDS];
static int next_client_fd = 3;  // Start after stdin/stdout/stderr

void init_fd_mapping(void) {
    for (int i = 0; i < MAX_FDS; i++) {
        fd_mapping[i] = -1;  // -1 means unmapped
    }
}

int add_fd_mapping(int server_fd) {
    int client_fd = next_client_fd++;
    fd_mapping[client_fd] = server_fd;
    return client_fd;
}

void remove_fd_mapping(int client_fd) {
    if (client_fd >= 0 && client_fd < MAX_FDS) {
        fd_mapping[client_fd] = -1;
    }
}

int get_server_fd(int client_fd) {
    if (client_fd < 0 || client_fd >= MAX_FDS) {
        return -1;
    }
    return fd_mapping[client_fd];
}
```

**Lifecycle**:
1. **open()**: Server gets real FD (e.g., 5), creates mapping 3→5, returns 3 to client
2. **read(3)**: Client sends 3, server looks up 3→5, reads from FD 5
3. **write(3)**: Client sends 3, server looks up 3→5, writes to FD 5
4. **close(3)**: Client sends 3, server looks up 3→5, closes FD 5, removes mapping

## Data Flow Example

### Complete Flow: Client writes "Hello" to a file

```
Client Program
    ↓
1. write(3, "Hello", 5)
    ↓
[LD_PRELOAD intercepts]
    ↓
2. intercept_write.h:write()
   - Checks reentry guard: OK
   - Gets RPC client: get_rpc_client() returns existing CLIENT*
   - Creates write_request:
     {
       fd: 3,
       data: {
         data_val: "Hello",
         data_len: 5
       },
       count: 5
     }
    ↓
3. syscall_write_1(&req, client)
   [Generated stub in protocol_clnt.c]
   - Serializes with xdr_write_request():
     * fd: 00 00 00 03 (4 bytes)
     * data_len: 00 00 00 05 (4 bytes)
     * data: 48 65 6C 6C 6F 00 00 00 (5 bytes + 3 padding)
     * count: 00 00 00 05 (4 bytes)
     Total: 20 bytes
   - Sends via clnt_call()
    ↓
        [Network: XDR binary data]
    ↓
4. Server receives request
   [Generated dispatcher in protocol_svc.c]
   - syscall_prog_1() receives packet
   - Identifies procedure: SYSCALL_WRITE (4)
   - Deserializes with xdr_write_request()
   - Calls syscall_write_1_svc()
    ↓
5. syscall_write_1_svc(&req, rqstp)
   [Server implementation in rpc_server.c]
   - Maps client_fd 3 → server_fd 5
   - Calls: write(5, "Hello", 5)
   - Gets result: 5 bytes written
   - Creates response:
     {
       result: 5,
       err: 0
     }
   - Returns &res
    ↓
6. Dispatcher serializes response
   - xdr_write_response():
     * result: 00 00 00 05
     * err: 00 00 00 00
     Total: 8 bytes
   - Sends back to client
    ↓
        [Network: XDR binary data]
    ↓
7. Client stub receives response
   - Deserializes with xdr_write_response()
   - Returns pointer to write_response
    ↓
8. Interceptor processes response
   - Extracts: result = 5, errno = 0
   - Clears reentry guard
   - Returns 5 to program
    ↓
Client Program
   - Sees: write() returned 5
```

## Design Decisions

### Why ONC RPC Instead of Raw Sockets?

| Aspect | Raw Sockets | ONC RPC | Winner |
|--------|-------------|---------|--------|
| **Serialization** | Manual memcpy, endianness handling | Automatic with XDR | RPC |
| **Code generation** | Write all client/server boilerplate | rpcgen generates stubs | RPC |
| **Protocol evolution** | Manual versioning | Built-in version support | RPC |
| **Error handling** | Custom protocol | Standard RPC error codes | RPC |
| **Transport flexibility** | Hard-coded socket type | Abstracted (UNIX/TCP/UDP) | RPC |
| **Industry standard** | Custom protocol | RFC 5531 standard | RPC |

**Conclusion**: ONC RPC provides robust, maintainable foundation with minimal manual code.

### Why LD_PRELOAD Instead of ptrace?

| Aspect | ptrace | LD_PRELOAD | Winner |
|--------|--------|------------|--------|
| **Performance** | Slow (context switch per syscall) | Fast (in-process) | LD_PRELOAD |
| **Complexity** | Complex state machine | Simple function override | LD_PRELOAD |
| **Privileges** | Requires root/capabilities | User-level | LD_PRELOAD |
| **Portability** | Kernel-dependent | Works on all UNIX | LD_PRELOAD |
| **Limitations** | Can intercept all syscalls | Only libc wrappers | ptrace |

**Conclusion**: LD_PRELOAD is simpler, faster, and sufficient for our use case (file I/O).

### Why Thread-Local RPC Connections?

**Problem**: Multiple threads calling syscalls simultaneously

**Option 1: Single shared CLIENT* with mutex**
- ❌ Serializes all RPC calls (performance bottleneck)
- ❌ Deadlock risk if RPC call triggers another intercepted syscall

**Option 2: Per-thread CLIENT* (chosen)**
- ✅ Parallel RPC calls
- ✅ No locking needed
- ✅ Clean thread-local state
- ⚠️ More connections (acceptable for typical use)

### Why Static Return Values in Server Handlers?

```c
open_response* syscall_open_1_svc(...) {
    static open_response res;  // Why static?
    // ...
    return &res;
}
```

**Reason**: RPC dispatcher serializes *after* the function returns. Stack-allocated variables would be destroyed, causing undefined behavior.

**Alternatives considered**:
- ❌ Stack allocation: Dangling pointer
- ❌ malloc(): Memory leak (who frees it?)
- ✅ Static: Simple, safe, sufficient (server is single-threaded per client)

### Why FD Mapping Instead of Direct FD Passing?

**Alternative**: Use SCM_RIGHTS to pass actual file descriptors over UNIX sockets

**Why not chosen**:
- ❌ UNIX sockets only (no TCP support)
- ❌ Kernel-dependent behavior
- ❌ Complex lifecycle management
- ❌ Doesn't work across machines

**FD mapping**:
- ✅ Transport-agnostic
- ✅ Works across network
- ✅ Simple implementation
- ✅ Complete control over FD lifecycle

## Error Handling Strategy

### Client-Side Error Handling

```
RPC Call Failed?
    ↓
[YES] ← clnt_call returns NULL
    ↓
Fallback to direct syscall
syscall(SYS_open, ...)
    ↓
Return to program

[NO] ← Got response
    ↓
Check res->result
    ↓
Set errno = res->err
    ↓
Return res->result
```

**Benefits**:
- Graceful degradation if server unavailable
- Transparent to user program
- errno propagated correctly

### Server-Side Error Handling

```c
open_response* syscall_open_1_svc(open_request *req, ...) {
    static open_response res;

    int server_fd = open(req->path, req->flags, req->mode);

    if (server_fd >= 0) {
        // Success path
        int client_fd = add_fd_mapping(server_fd);
        res.fd = client_fd;
        res.result = client_fd;
        res.err = 0;
    } else {
        // Error path
        res.fd = -1;
        res.result = -1;
        res.err = errno;  // Capture immediately!
    }

    return &res;
}
```

**Key**: Capture errno *immediately* after syscall, before any other function calls.

## Thread Safety Analysis

### Per-Syscall Reentry Guards
```c
static __thread int in_open_intercept = 0;
```
- Prevents recursion within same interceptor
- Thread-local: Each thread has independent guard
- Cleared before returning

### Global RPC Progress Flag
```c
static __thread int rpc_in_progress = 0;
```
- Set during RPC initialization and calls
- Checked by ALL interceptors
- Prevents RPC library's internal syscalls from being intercepted

### RPC Client Handle
```c
static __thread CLIENT *rpc_client = NULL;
```
- One connection per thread
- Lazy initialized on first use
- Persistent across syscalls

### Thread Safety Guarantee

✅ **Safe**: Multiple threads can simultaneously:
- Call intercepted syscalls
- Make RPC calls (independent connections)
- Manage FD mappings (server-side, protected by process model)

⚠️ **Note**: FD namespace is shared across threads (by design, matches POSIX behavior)

## Performance Considerations

### Overhead Per Syscall

```
Component                  Overhead
────────────────────────────────────
Interception check         ~10 ns   (guard check)
RPC serialization          ~500 ns  (XDR encoding)
UNIX socket round-trip     ~2-3 μs  (same machine)
TCP socket round-trip      ~50-100 μs (localhost)
Server processing          ~1 μs    (FD lookup + syscall)
Response deserialization   ~500 ns  (XDR decoding)
────────────────────────────────────
Total (UNIX):              ~5 μs    per syscall
Total (TCP):               ~50 μs   per syscall
```

**Comparison to direct syscall**: ~100-200 ns

**Implication**: 25-500x overhead. Acceptable for:
- Testing/development
- Sandboxing (security > performance)
- Remote file access (network latency dominates)

**Not suitable for**:
- High-frequency I/O (millions of syscalls/sec)
- Latency-critical applications

### Memory Usage

```
Component              Memory
────────────────────────────────
Client (per thread):   ~4 KB    (RPC client handle)
Server (global):       ~4 KB    (FD mapping table)
Message buffer:        1 MB     (max read/write size)
────────────────────────────────
Total (typical):       ~5 MB
```

## Security Considerations

### Current Security Model

⚠️ **No Authentication**: Any client can connect to server

⚠️ **No Encryption**: All data in plaintext

⚠️ **No Access Control**: Server executes all requests with its privileges

⚠️ **FD Leakage**: Improper client termination may leak server FDs

### Suitable Use Cases

✅ **Safe**:
- Development/testing on localhost
- Sandboxing with trusted server
- Single-user systems

❌ **Unsafe**:
- Production multi-user systems
- Untrusted networks
- Privileged servers (running as root)

### Future Enhancements

Possible security improvements:
- Authentication (client credentials)
- Encryption (TLS over TCP)
- Access control (file path filtering)
- Resource limits (max open FDs per client)
- Audit logging

---

**Next**: [04_CODE_STRUCTURE.md](./04_CODE_STRUCTURE.md) - Detailed file-by-file breakdown

**Prev**: [01_INTRODUCTION.md](./01_INTRODUCTION.md) - Project introduction
